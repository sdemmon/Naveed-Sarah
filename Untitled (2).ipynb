{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9253f2c455ccd169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:42:02.670886Z",
     "start_time": "2024-04-17T12:42:02.647897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 19:31:06.396966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time \n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231e5e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:08.028904Z",
     "start_time": "2024-04-17T12:49:08.002920Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextClassificationPipeline' from 'transformers' (/Users/sarahdemmon/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistilBertTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFDistilBertForSequenceClassification\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextClassificationPipeline\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TextClassificationPipeline' from 'transformers' (/Users/sarahdemmon/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# test the bert model \n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "# train the model using Naive Bayes model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import joblib\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopw = stopwords.words('english')\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly.offline import iplot\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141388b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:12.843320Z",
     "start_time": "2024-04-17T12:49:11.832766Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('booksummaries.txt', sep='\\t',\n",
    "                 names=['id1', 'id2', 'title', 'author', 'year', 'genres', 'summary'],\n",
    "                 usecols=[2, 3, 5, 6], converters={'genres' : lambda s : (list(json.loads(s).values()) if s else None)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e1284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:16.354331Z",
     "start_time": "2024-04-17T12:49:16.313355Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e1222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:16.384316Z",
     "start_time": "2024-04-17T12:49:16.357330Z"
    }
   },
   "outputs": [],
   "source": [
    "all_genres = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2c9dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:20.529833Z",
     "start_time": "2024-04-17T12:49:20.505846Z"
    }
   },
   "outputs": [],
   "source": [
    "genres_column = df['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41370bd0674baa54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:20.559823Z",
     "start_time": "2024-04-17T12:49:20.537827Z"
    }
   },
   "outputs": [],
   "source": [
    "genres_column.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732f1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:25.242454Z",
     "start_time": "2024-04-17T12:49:25.215472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through each row to extract and split the list of genres\n",
    "all_genres = []\n",
    "# Iterate through each element in the 'genres_column'\n",
    "for genres_item in genres_column:\n",
    "    # Check if the element is a string\n",
    "    if isinstance(genres_item, str):\n",
    "        # Split the string by comma and extend the list of all genres\n",
    "        all_genres.extend([genre.strip() for genre in genres_item.split(',')])\n",
    "    # Check if the element is a list\n",
    "    elif isinstance(genres_item, list):\n",
    "        # Extend the list of all genres directly\n",
    "        all_genres.extend(genres_item)\n",
    "\n",
    "# Count the occurrences of each unique genre\n",
    "genre_counts = Counter(all_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129756bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:25.257453Z",
     "start_time": "2024-04-17T12:49:25.246453Z"
    }
   },
   "outputs": [],
   "source": [
    "print(genre_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c0214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:28.886859Z",
     "start_time": "2024-04-17T12:49:28.862873Z"
    }
   },
   "outputs": [],
   "source": [
    "all_genres_series = pd.Series(all_genres)\n",
    "genre_counts = all_genres_series.value_counts()\n",
    "\n",
    "# Now you can use genre_counts as desired\n",
    "print(genre_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3d11d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:28.901851Z",
     "start_time": "2024-04-17T12:49:28.892856Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_genres = [#'Speculative fiction',\n",
    "                'Science Fiction','Crime Fiction','Non-fiction','Children\\'s literature',\n",
    "                'Fantasy', 'Mystery', 'Suspense', 'Young adult literature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541e484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:39.649030Z",
     "start_time": "2024-04-17T12:49:32.009582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through each row to extract and split the list of genres\n",
    "book_list = []\n",
    "# Iterate through each element in the 'genres_column'\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"genres\"]:\n",
    "        for genre in valid_genres:\n",
    "            new_row={\n",
    "                \"title\": row[\"title\"],\n",
    "                \"author\": row[\"author\"],\n",
    "                \"summary\": row[\"summary\"]\n",
    "                }\n",
    "            genre_list=row[\"genres\"]\n",
    "            if genre in genre_list:\n",
    "                new_row[\"genre\"]=genre\n",
    "                book_list.append(new_row)\n",
    "book_df = pd.DataFrame(book_list)\n",
    "book_df  # need to drop the duplicates up here\n",
    "#     # Check if the element is a string\n",
    "#     if isinstance(genres_item, str):\n",
    "#         # Split the string by comma and extend the list of all genres\n",
    "#         all_genres.extend([genre.strip() for genre in genres_item.split(',')])\n",
    "#     # Check if the element is a list\n",
    "#     elif isinstance(genres_item, list):\n",
    "#         # Extend the list of all genres directly\n",
    "#         all_genres.extend(genres_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66af8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:39.680014Z",
     "start_time": "2024-04-17T12:49:39.660028Z"
    }
   },
   "outputs": [],
   "source": [
    "book_df[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18b8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:39.712992Z",
     "start_time": "2024-04-17T12:49:39.689006Z"
    }
   },
   "outputs": [],
   "source": [
    "book_df[\"title\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7cbe48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:40.347174Z",
     "start_time": "2024-04-17T12:49:39.839457Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=book_df, order=book_df[\"genre\"].value_counts().index, y='genre')\n",
    "plt.title(\"Counts per Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf87be9ba55e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:46.625598Z",
     "start_time": "2024-04-17T12:49:46.602611Z"
    }
   },
   "outputs": [],
   "source": [
    "# encode categories - easy encoding\n",
    "book_df['encoded_text'] = book_df['genre'].astype('category').cat.codes\n",
    "\n",
    "book_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2857f75f9939d1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:49:50.126888Z",
     "start_time": "2024-04-17T12:49:50.115894Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract data - x_data\n",
    "data_summary = book_df['summary'].to_list()\n",
    "\n",
    "# data labels - y_data\n",
    "data_labels = book_df['encoded_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b885b88775a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_summaries, val_summaries, train_labels, val_labels = train_test_split(data_summary, data_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "train_summaries, test_summaries, train_labels, test_labels = train_test_split(train_summaries, train_labels, test_size=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b877696f1424564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "benchmarks = {'NB' : [0.0, 0.0, 0.0],\n",
    "              'NB_tuned':  [0.0, 0.0, 0.0],\n",
    "              'SVC' :  [0.0, 0.0, 0.0],\n",
    "              'SVC_tuned':  [0.0, 0.0, 0.0],\n",
    "              'LR' :  [0.0, 0.0, 0.0],\n",
    "              'LR_tuned':  [0.0, 0.0, 0.0],\n",
    "                }\n",
    "t0 = time()\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,1))),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear'), n_jobs=1)),\n",
    "            ])\n",
    "LogReg_pipeline.fit(train_summaries, train_labels)\n",
    "benchmarks['LR'][0] = (time() - t0)/60\n",
    "#print(\"Training complete! Saving trained model....\")\n",
    "filename = \"./LogReg_model.sav\"\n",
    "joblib.dump(LogReg_pipeline, filename)\n",
    "print(\"Training took: {:.3f}[seconds] to complete and has been saved as {}\".format(benchmarks['LR'][0],filename))\n",
    "print(\"####Before tuning:####\")\n",
    "print('Train Accuracy : %.3f'%LogReg_pipeline.score(train_summaries, train_labels))\n",
    "print('Test Accuracy : %.3f'%LogReg_pipeline.score(test_summaries, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb4a21e75dafd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:53:30.975399Z",
     "start_time": "2024-04-17T12:53:14.185301Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8327a68b7c4f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:05:41.569222Z",
     "start_time": "2024-04-17T11:03:05.639786Z"
    }
   },
   "outputs": [],
   "source": [
    "# BERT MODEL\n",
    "# define the model \n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# encode x inputs\n",
    "train_encodings = tokenizer(train_summaries, truncation=True, padding=True)\n",
    "\n",
    "# encode y ouputs\n",
    "val_encodings = tokenizer(val_summaries, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442de383f502dd31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:09:44.163917Z",
     "start_time": "2024-04-17T11:09:12.841039Z"
    }
   },
   "outputs": [],
   "source": [
    "# make the dataset \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),train_labels))\n",
    "\n",
    "# make y dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600dae8df786ad27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:32:02.729484Z",
     "start_time": "2024-04-17T11:32:01.192117Z"
    }
   },
   "outputs": [],
   "source": [
    "#from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "from transformers import TFDistilBertForSequenceClassification, AdamWeightDecay, create_optimizer\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=8)\n",
    "\n",
    "num_train_epochs = 7\n",
    "per_device_train_batch_size = 16  \n",
    "per_device_eval_batch_size = 64\n",
    "warmup_steps = 500\n",
    "weight_decay = 1e-5 \n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataset) * num_train_epochs\n",
    "\n",
    "optimizer, lr_scheduler = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_train_steps=total_steps,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    weight_decay_rate=weight_decay\n",
    ")\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch['input_ids'], attention_mask=batch['attention_mask'], training=True)[0]\n",
    "        loss = loss_fn(batch['labels'], logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_accuracy.update_state(batch['labels'], logits)\n",
    "    return loss\n",
    "\n",
    "# Execute training\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f'Start of epoch {epoch}')\n",
    "    for batch in train_dataset:\n",
    "        loss = train_step(batch)\n",
    "        print(f'Step loss: {loss.numpy()}')\n",
    "\n",
    "    # Validation loop can be added here similarly using model and val_dataset\n",
    "    # Reset metrics at the end of each epoch\n",
    "    train_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac4401852c78ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:24:29.326612Z",
     "start_time": "2024-04-17T11:24:29.068286Z"
    }
   },
   "outputs": [],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa8ad47b9a4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a52fbffb2c9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50764e73428833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:12:45.409954Z",
     "start_time": "2024-04-17T11:12:45.260284Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0538d591a8e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T04:25:50.513771Z",
     "start_time": "2024-04-17T04:25:49.395885Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "book_df['summary'] = book_df['summary'].fillna('')\n",
    "book_df['summary_genre'] = book_df['summary'] + ' ' + book_df['genre']  # attaching the genre to the summary\n",
    "print(book_df.iloc[0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747337f996c5ed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T04:26:15.926458Z",
     "start_time": "2024-04-17T04:26:10.977770Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf.fit_transform(book_df['summary_genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc2905146e9ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T04:26:17.719822Z",
     "start_time": "2024-04-17T04:26:17.645632Z"
    }
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "genre_matrix = mlb.fit_transform(book_df['genre']) # binarized genre matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313517b7a6c976c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T04:26:20.207456Z",
     "start_time": "2024-04-17T04:26:20.147889Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_genre_matrix = hstack((tfidf_matrix, genre_matrix))  # append so we can get a nice matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae30616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T04:29:39.918461Z",
     "start_time": "2024-04-17T04:29:31.409562Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarity = linear_kernel(tfidf_genre_matrix)\n",
    "print(type(cosine_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d3c365b85426a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T04:34:42.142006Z",
     "start_time": "2024-04-17T04:34:42.110525Z"
    }
   },
   "outputs": [],
   "source": [
    "indices = pd.Series(book_df.index, index=book_df['title']).drop_duplicates()  # cant drop the duplicates here\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be2480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T04:34:45.283683Z",
     "start_time": "2024-04-17T04:34:45.264476Z"
    }
   },
   "outputs": [],
   "source": [
    "def recommend(title, similarity_matrix, topk=10):\n",
    "    book_index: int = indices[title]  # will return an int\n",
    "    # enumerate here adds index of book being referenced by similarity score, \n",
    "    # allows reverse lookup once sorted by similarity score\n",
    "    similarity_scores = list(enumerate(similarity_matrix[book_index]))  # enumerate of ?\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    top_scores = similarity_scores[1:topk+1] # index 0 should be original \n",
    "    recommendation_indices = [i[0] for i in top_scores]\n",
    "    return book_df['title'].iloc[recommendation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cd6e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T04:34:49.011514Z",
     "start_time": "2024-04-17T04:34:48.911931Z"
    }
   },
   "outputs": [],
   "source": [
    "recs = recommend('Animal Farm', cosine_similarity)\n",
    "book_df.iloc[recs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69cb6519f3c49a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T06:09:12.627316Z",
     "start_time": "2024-04-17T06:09:12.432587Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "train, test = train_test_split(book_df, random_state=42, test_size=0.2, shuffle=True, stratify=book_df['genre'])\n",
    "\n",
    "print(train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4603ceb09a458e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T06:09:03.816361Z",
     "start_time": "2024-04-17T06:09:03.786325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting features and labels for training and testing\n",
    "train_x = train.summary\n",
    "train_y = train.genre\n",
    "test_x = test.summary.to_numpy()\n",
    "test_y = test.genre.to_numpy()\n",
    "print(train_y)  # train based on genre and summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28f7390ceaee81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T06:03:35.557571Z",
     "start_time": "2024-04-17T06:03:26.407543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 10000    # Max size of vocabulary\n",
    "max_length = 100      # Max length of each sequence\n",
    "batch_size = 32\n",
    "embedding_dims = 10\n",
    "filters = 16\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # build word index\n",
    "tokenizer.fit_on_texts(train_x)  # creates dictionary mapping each word to an int\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(train_x)  # converts text to a list of ints \n",
    "test_sequences = tokenizer.texts_to_sequences(test_x)       # converts text to a list of ints\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length)  # make sure rows have same length\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4bfd3fa384b00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T06:03:37.665855Z",
     "start_time": "2024-04-17T06:03:37.642867Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_labels = encoder.fit_transform(train_y) # transform labels into numeric format\n",
    "test_labels = encoder.transform(test_y)  # transform into numeric format\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels)  # creates the matrix for the labels \n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0e4530087bb1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T06:06:44.243510Z",
     "start_time": "2024-04-17T06:06:44.156198Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, GlobalAveragePooling1D\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=max_length),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_labels.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c851e55bbb17e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T05:50:41.643632Z",
     "start_time": "2024-04-17T05:43:46.510619Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_padded, train_labels,\n",
    "    epochs=100,\n",
    "    validation_data=(test_padded, test_labels),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406216a5f88a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T05:51:15.454300Z",
     "start_time": "2024-04-17T05:51:14.770578Z"
    }
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_padded, test_labels, verbose=2)\n",
    "print(\"Test Loss, Test Accuracy:\", results)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"text_classification_cnn_model.h5\")\n",
    "print(\"Model saved in HDF5 format.\")\n",
    "\n",
    "# Print the last accuracy and validation accuracy\n",
    "final_accuracy = history.history['accuracy'][-1]\n",
    "final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "print(f\"Final Training Accuracy: {final_accuracy*100:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {final_val_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483ae6f8140c407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T05:36:03.107738Z",
     "start_time": "2024-04-17T05:36:01.379720Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22ebc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T05:35:57.853430Z",
     "start_time": "2024-04-17T05:35:57.288507Z"
    }
   },
   "outputs": [],
   "source": [
    "# Printing the size of the training and testing datasets\n",
    "print(\"Training dataset = {}\".format(len(train_x)))\n",
    "print(\"Testing dataset = {}\".format(len(test_x)))\n",
    "\n",
    "# Visualizing the distribution of genres in the training and testing datasets\n",
    "colors=['tab:orange', 'tab:green', 'tab:blue', 'tab:brown', 'tab:pink', 'tab:purple','tab:red', 'tab:gray', 'tab:olive']\n",
    "plt.figure(figsize=(25,8))\n",
    "plt.subplot(1,2,1).set_title(\"Train Dataset-Counts per Genre\")\n",
    "train.groupby('genre').size().sort_values(ascending=True).plot(kind='barh', color=colors,ax=plt.gca())\n",
    "plt.subplot(1,2,2).set_title(\"Test Dataset-Counts per Genre\")\n",
    "test.groupby('genre').size().sort_values(ascending=True).plot(kind='barh', color=colors,ax=plt.gca())\n",
    "plt.show()\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4094c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T05:36:23.237214Z",
     "start_time": "2024-04-17T05:36:14.190447Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model using Naive Bayes model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import joblib\n",
    "# Define and train your model using the training data\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,1), use_idf=True)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB())),\n",
    "            ])\n",
    "NB_pipeline.fit(train_x, train_y)\n",
    "\n",
    "# Save the trained model\n",
    "filename = \"./NB_model.sav\"\n",
    "joblib.dump(NB_pipeline, filename)\n",
    "print(\"Model has been saved as {}\".format(filename))\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "print(\"#### Model Evaluation on Test Data: ####\")\n",
    "print('Train Accuracy : %.3f'%NB_pipeline.score(train_x, train_y))\n",
    "print('Test Accuracy : %.3f'%NB_pipeline.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##merge a few categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62adcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bin genres into 10 types (..) with strings that contain \n",
    "##fantasy = '\\WFantasy\\W'\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for genres_item in genres_column:\n",
    "    if isinstance(genres_item, str):\n",
    "        # Split the string by comma and extend the list of all genres\n",
    "        all_genres.extend([genre.strip() for genre in genres_item.split(',')])\n",
    "        if "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
